# Parag - Next-Generation RAG System

A **self-learning**, **autonomous** RAG (Retrieval-Augmented Generation) system powered by the **Paradox ecosystem**. Parag eliminates heavy ML dependencies (PyTorch, FAISS, transformers) and replaces them with custom, self-learning frameworks.

## ğŸŒŸ Key Features

### Self-Learning Architecture (Paradox Ecosystem)
- **Paradma** - Self-learning mathematical operations (learns from NumPy, graduates to native Python)
- **ParadoxLF** - Autonomous memory engine with creative capabilities
- **modules.framework** - Custom PyTorch replacement with Tensor and autograd
- **HyperMatrix** - Quantum-like superposition for uncertain knowledge

### Core Capabilities
- **Minimal Dependencies**: Only ~15MB (NumPy + PyPDF2 + tqdm) vs ~2GB with traditional stack
- **Progressive Learning**: Operations start with NumPy, evolve to native implementations
- **Creative Features**: Concept blending via `imagine()`, temporal prediction
- **Quantum-Like Reasoning**: Superposition support for conflicting facts
- **Autonomous Optimization**: Memory engine evolves independently
- **Full Explainability**: Deterministic responses without LLM when appropriate

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/ethcocoder/parag.git
cd parag

# Install minimal dependencies
pip install -r requirements.txt

# Ensure Paradox ecosystem is available
# (Paradma, ParadoxLF, modules should be in parent directory)
```

### Total Install Size
- **Before**: ~2GB (PyTorch + FAISS + transformers)
- **After**: ~15MB (NumPy + PyPDF2 + tqdm)
- **Reduction**: 98.5% smaller! ğŸ‰

## ğŸš€ Quick Start (Paradox-Powered)

```python
from parag.core import KnowledgeUnit
from parag.embeddings import ParadoxEmbeddings  # Self-learning embeddings!
from parag.vectorstore import ParadoxVectorStore  # Autonomous memory
from parag.retrieval import Retriever
from parag.reasoning import StateManager
from parag.generation import DeterministicGenerator

# 1. Ingest documents
documents = load_document("path/to/document.pdf")
chunked_docs = chunk_documents(documents)

# 2. Create embeddings
embedding_model = SentenceTransformerEmbeddings()
embedding_dim = embedding_model.get_embedding_dim()

# 3. Initialize vector store
vector_store = FAISSVectorStore(dimension=embedding_dim)

# 4. Create knowledge units and add to retrieval system
retriever = Retriever(embedding_model, vector_store)

units = [
    KnowledgeUnit(
        content=doc["content"],
        metadata=doc["metadata"]
    )
    for doc in chunked_docs
]

retriever.add_knowledge_units(units)

# 5. Retrieve relevant information
query = "What is the main topic?"
result = retriever.retrieve(query, top_k=5)

# 6. Build state and reason
state_manager = StateManager()
state = state_manager.build_from_retrieval(result)
state.detect_conflicts()

# 7. Generate response
generator = DeterministicGenerator()
response = generator.generate_from_state(query, state)
print(response)
```

## ğŸ“¦ Architecture

```
parag/
â”œâ”€â”€ core/                # Core data models
â”‚   â”œâ”€â”€ knowledge_unit.py
â”‚   â”œâ”€â”€ retrieval_result.py
â”‚   â””â”€â”€ rag_state.py
â”œâ”€â”€ ingestion/          # Document processing
â”‚   â”œâ”€â”€ loaders.py
â”‚   â”œâ”€â”€ chunker.py
â”‚   â””â”€â”€ metadata.py
â”œâ”€â”€ embeddings/         # Embedding generation
â”‚   â”œâ”€â”€ base.py
â”‚   â””â”€â”€ sentence_transformer.py
â”œâ”€â”€ vectorstore/        # Vector database
â”‚   â”œâ”€â”€ faiss_store.py
â”‚   â””â”€â”€ index_manager.py
â”œâ”€â”€ retrieval/          # Retrieval engine
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ ranker.py
â”œâ”€â”€ reasoning/          # Reasoning layer
â”‚   â”œâ”€â”€ state_manager.py
â”‚   â”œâ”€â”€ conflict_detector.py
â”‚   â””â”€â”€ uncertainty.py
â””â”€â”€ generation/         # Response generation
    â”œâ”€â”€ prompt_builder.py
    â””â”€â”€ llm_adapter.py
```

## ğŸ¯ Core Concepts

### KnowledgeUnit
All retrieved data is wrapped in a standard structure:
```python
KnowledgeUnit:
  content         # text / image / tensor
  embedding       # vector representation
  metadata        # source, timestamp, tags
  confidence      # optional confidence score
```

### RAGState
Internal state representation for reasoning:
```python
RAGState:
  facts           # aggregated facts from knowledge units
  knowledge_units # all contributing units
  conflicts       # detected contradictions
  uncertainty     # uncertainty measurement
```

### RetrievalResult
Structured container for retrieval outputs:
```python
RetrievalResult:
  units           # list of KnowledgeUnits
  scores          # relevance scores
  query           # original query
  metadata        # retrieval metadata
```

## ğŸ›£ï¸ Roadmap

### âœ… Phase 1-4: Foundation (Current)
- [x] Classic RAG foundation
- [x] Structured retrieval with KnowledgeUnit
- [x] RAG state and reasoning layer
- [x] Deterministic generation

### ğŸ”œ Phase 5-7: Advanced Features (Future)
- [ ] Concept blending hooks
- [ ] Temporal retrieval
- [ ] Paradma law-based reasoning
- [ ] Entropy thresholds
- [ ] Curiosity-driven re-query
- [ ] Human feedback ingestion

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# With coverage
pytest tests/ --cov=parag
```

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ”— Links

- **Documentation**: See `doc/` directory
- **Roadmap**: `doc/roadmap.md`
- **Todo**: `doc/todo.md`

## ğŸ’¡ Philosophy

Parag is built on the principle that **reasoning should happen in structured state, not in prompts**. By separating retrieval, reasoning, and generation:

1. **Retrieval** finds relevant knowledge
2. **Reasoning** operates on structured facts
3. **Generation** is the final, optional step

This enables:
- **Transparency**: Every decision is traceable
- **Scalability**: Each layer can evolve independently
- **Reliability**: Deterministic behavior when needed
- **Future-proofing**: Ready for advanced cognitive engines

---

**Built with â¤ï¸ by ethcocoder**
